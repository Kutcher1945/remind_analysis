{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b943f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#ðŸ“Ž You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92531ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Set up device and initialize random seed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Path to the Alzheimer's dataset\n",
    "DATA_ROOT = Path(\"/kaggle/input/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543874c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define class mapping for Alzheimer's dataset\n",
    "class_map = {\n",
    "    \"No Impairment\": 0,\n",
    "    \"Very Mild Impairment\": 1,\n",
    "    \"Mild Impairment\": 2,\n",
    "    \"Moderate Impairment\": 3\n",
    "}\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Loop over train and test directories\n",
    "for folder in [\"train\", \"test\"]:\n",
    "    folder_path = DATA_ROOT / folder\n",
    "    for class_name, class_idx in class_map.items():\n",
    "        class_dir = folder_path / class_name\n",
    "        if class_dir.is_dir():\n",
    "            for img_path in class_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "                    image_paths.append(str(img_path))\n",
    "                    labels.append(class_idx)\n",
    "\n",
    "print(\"Total images found:\", len(image_paths))\n",
    "\n",
    "# Create a DataFrame for the dataset\n",
    "data_df = pd.DataFrame({\n",
    "    \"path\": image_paths,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4b135",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Image size for Alzheimer's classification (MRI images might require different image size)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 4\n",
    "N_SPLITS = 5  # 5-fold cross-validation\n",
    "\n",
    "# Data augmentations (adjusted for MRI data)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58330c98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AlzheimerDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, \"path\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ea76b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(num_classes=NUM_CLASSES):\n",
    "    # Pretrained Xception model from timm\n",
    "    model = timm.create_model(\n",
    "        \"xception\",\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4dc360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0faf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "fold_histories = []\n",
    "fold_best_accs = []\n",
    "fold_best_paths = []\n",
    "\n",
    "print(f\"Starting {N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    skf.split(data_df[\"path\"], data_df[\"label\"]), 1\n",
    "):\n",
    "    print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "\n",
    "    train_df = data_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = data_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    print(\"Train size:\", len(train_df))\n",
    "    print(\"Val size:\", len(val_df))\n",
    "\n",
    "    train_dataset = AlzheimerDataset(train_df, transform=train_transform)\n",
    "    val_dataset = AlzheimerDataset(val_df, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = create_model()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": []\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_path = f\"best_xception_alzheimer_fold{fold}.pth\"\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold} | Epoch [{epoch}/{NUM_EPOCHS}] \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model (highest validation accuracy for this fold)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  -> New best model for fold {fold} saved with val_acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    fold_histories.append(history)\n",
    "    fold_best_accs.append(best_val_acc)\n",
    "    fold_best_paths.append(best_model_path)\n",
    "\n",
    "print(\"\\nCross-validation complete.\")\n",
    "for i, acc in enumerate(fold_best_accs, 1):\n",
    "    print(f\"Fold {i} best val_acc: {acc:.4f}\")\n",
    "print(f\"Mean val_acc over {N_SPLITS} folds: {np.mean(fold_best_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e643cf2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pick the fold with the highest validation accuracy\n",
    "best_fold_index = int(np.argmax(fold_best_accs))  # 0-based index\n",
    "best_fold = best_fold_index + 1\n",
    "best_history = fold_histories[best_fold_index]\n",
    "\n",
    "print(f\"Best fold is Fold {best_fold} with val_acc = {fold_best_accs[best_fold_index]:.4f}\")\n",
    "\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, best_history[\"train_acc\"], label=f\"Fold {best_fold} Train Acc\")\n",
    "plt.plot(epochs, best_history[\"val_acc\"], label=f\"Fold {best_fold} Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"Train vs Val Accuracy (Fold {best_fold})\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, best_history[\"train_loss\"], label=f\"Fold {best_fold} Train Loss\")\n",
    "plt.plot(epochs, best_history[\"val_loss\"], label=f\"Fold {best_fold} Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Train vs Val Loss (Fold {best_fold})\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a029263",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate model using confusion matrix and classification report\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data_df[\"path\"], data_df[\"label\"]), 1):\n",
    "    val_df = data_df.iloc[val_idx].reset_index(drop=True)\n",
    "    val_dataset = AlzheimerDataset(val_df, transform=val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    best_model_path = f\"best_xception_alzheimer_fold{fold}.pth\"\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(class_map.keys())))\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cm, classes=list(class_map.keys()), normalize=False, title=\"Confusion Matrix (Counts)\")\n",
    "plot_confusion_matrix(cm, classes=list(class_map.keys()), normalize=True, title=\"Confusion Matrix (Normalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652c858",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM implementation for Xception model.\n",
    "    Target layer: 'block12.rep.4' (last convolutional layer before fully connected layers).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_name=\"block12.rep.4\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = dict([*self.model.named_modules()])[target_layer_name]\n",
    "\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # forward hook: save activations\n",
    "        self.forward_hook = self.target_layer.register_forward_hook(self._forward_hook)\n",
    "        # backward hook: save gradients\n",
    "        self.backward_hook = self.target_layer.register_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, input, output):\n",
    "        # output shape: [N, C, H, W]\n",
    "        self.activations = output\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        # grad_output[0] shape: [N, C, H, W]\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        \"\"\"\n",
    "        input_tensor: (1, 3, H, W)\n",
    "        class_idx: target class index; if None, use predicted class\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        loss = output[0, class_idx]\n",
    "        loss.backward()\n",
    "\n",
    "        # activations & gradients with batch dimension\n",
    "        # shapes: [N, C, H, W]; we use only the first sample (N=1)\n",
    "        activations = self.activations[0].detach()   # [C, H, W]\n",
    "        gradients = self.gradients[0].detach()       # [C, H, W]\n",
    "\n",
    "        # global average pooling on gradients -> [C, 1, 1]\n",
    "        weights = torch.mean(gradients, dim=(1, 2), keepdim=True)\n",
    "        # weighted sum of activations -> [H, W]\n",
    "        cam = torch.sum(weights * activations, dim=0)\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-8)\n",
    "\n",
    "        cam_np = cam.cpu().numpy()  # shape: [H, W]\n",
    "        return cam_np\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.forward_hook.remove()\n",
    "        self.backward_hook.remove()\n",
    "\n",
    "\n",
    "def show_gradcam_example(model, dataset, idx=0, target_layer=\"block12.rep.4\"):\n",
    "    \"\"\"\n",
    "    Show Grad-CAM heatmap for one example from a dataset.\n",
    "    \"\"\"\n",
    "    # inverse normalization to display the image\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "\n",
    "    img_tensor, label = dataset[idx]\n",
    "    input_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # create Grad-CAM object\n",
    "    gradcam = GradCAM(model, target_layer_name=target_layer)\n",
    "    cam = gradcam.generate(input_tensor)\n",
    "    gradcam.remove_hooks()\n",
    "\n",
    "    # prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    # prepare original image for plotting\n",
    "    img_for_display = inv_normalize(img_tensor).clamp(0, 1).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # resize CAM to image size\n",
    "    cam_uint8 = (cam * 255).astype(np.uint8)          # shape [H, W]\n",
    "    cam_img = Image.fromarray(cam_uint8)              # grayscale image\n",
    "    cam_resized = cam_img.resize(\n",
    "        (img_for_display.shape[1], img_for_display.shape[0]),\n",
    "        resample=Image.BILINEAR\n",
    "    )\n",
    "    cam_resized = np.array(cam_resized) / 255.0       # back to [0, 1]\n",
    "\n",
    "    # Plot original, CAM, and overlay\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_for_display)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Original\\nTrue: {target_names[label]}\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(cam_resized, cmap=\"jet\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Grad-CAM\\nPred: {target_names[pred_idx]}\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_for_display)\n",
    "    plt.imshow(cam_resized, cmap=\"jet\", alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Overlay\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6121917",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define target names for the Alzheimer's classes\n",
    "target_names = [\"No Impairment\", \"Very Mild Impairment\", \"Mild Impairment\", \"Moderate Impairment\"]\n",
    "\n",
    "# Rebuild the best fold's validation dataset\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "best_fold_index = int(np.argmax(fold_best_accs))  # 0-based index\n",
    "best_fold = best_fold_index + 1\n",
    "\n",
    "current_fold = 1\n",
    "val_dataset_best_fold = None\n",
    "\n",
    "# Retrieve validation set for the best fold\n",
    "for train_idx, val_idx in skf.split(data_df[\"path\"], data_df[\"label\"]):\n",
    "    if current_fold == best_fold:\n",
    "        val_df_best = data_df.iloc[val_idx].reset_index(drop=True)\n",
    "        val_dataset_best_fold = AlzheimerDataset(val_df_best, transform=val_transform)\n",
    "        break\n",
    "    current_fold += 1\n",
    "\n",
    "# Load the best model from the best fold\n",
    "best_model_path = f\"best_xception_alzheimer_fold{best_fold}.pth\"\n",
    "best_model = create_model()\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "print(f\"Showing Grad-CAM examples from Fold {best_fold}\")\n",
    "\n",
    "# Pick at least one index for each class [\"No Impairment\", \"Very Mild Impairment\", \"Mild Impairment\", \"Moderate Impairment\"]\n",
    "num_classes = len(class_map)\n",
    "\n",
    "# Find the first occurrence index of each class in the validation dataset\n",
    "class_to_idx = {}  # label_int -> idx in dataset\n",
    "\n",
    "for idx in range(len(val_dataset_best_fold)):\n",
    "    _, label = val_dataset_best_fold[idx]\n",
    "    label_int = int(label)\n",
    "    if label_int not in class_to_idx:\n",
    "        class_to_idx[label_int] = idx\n",
    "    if len(class_to_idx) == num_classes:\n",
    "        break\n",
    "\n",
    "print(\"Chosen indices per class (label -> index):\", class_to_idx)\n",
    "\n",
    "# Run Grad-CAM on one example per class (if present in this fold)\n",
    "for label_int, idx in sorted(class_to_idx.items()):\n",
    "    class_name = list(class_map.keys())[label_int]\n",
    "    print(f\"\\nGrad-CAM for class '{class_name}' (label={label_int}) at validation index {idx} (Fold {best_fold})\")\n",
    "    show_gradcam_example(best_model, val_dataset_best_fold, idx=idx, target_layer=\"block12.rep.4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
